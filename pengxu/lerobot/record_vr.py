#!/usr/bin/env python3
# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Records a dataset. Actions for the robot can be either generated by teleoperation or by a policy.

Example:

```shell
python -m lerobot.record \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem58760431541 \
    --robot.cameras="{laptop: {type: opencv, camera_index: 0, width: 640, height: 480}}" \
    --robot.id=black \
    --dataset.repo_id=aliberts/record-test \
    --dataset.num_episodes=2 \
    --dataset.single_task="Grab the cube" \
    # <- Teleop optional if you want to teleoperate to record or in between episodes with a policy \
    # --teleop.type=so100_leader \
    # --teleop.port=/dev/tty.usbmodem58760431551 \
    # --teleop.id=blue \
    # <- Policy optional if you want to record with a policy \
    # --policy.path=${HF_USER}/my_policy \
```
"""

from rtde_control import RTDEControlInterface  # 机械臂控制[1](@ref)
from rtde_receive import RTDEReceiveInterface 
import rtde_control
import rtde_receive
import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from pprint import pformat
import numpy as np
import rerun as rr
import random
import threading 
# from rtde_control import RTDEControlInterface  # 机械臂控制[1](@ref)
# from rtde_receive import RTDEReceiveInterface  # 状态读取[1](@ref)
# from rtde_io import RTDEIOInterface  # 数字IO控制
import cv2  # 摄像头采集
#from lerobot.common.datasets.push_dataset_to_hub import push_to_hub  # 数据集上传[5](@ref)
import threading  # 添加线程支持
from TeleVision import OpenTeleVision
from Preprocessor import VuerPreprocessor
from constants_vuer import tip_indices
from dex_retargeting.retargeting_config import RetargetingConfig
from pytransform3d import rotations
import argparse
import yaml
from multiprocessing import Array, Process, shared_memory, Queue, Manager, Event, Semaphore
from typing import Optional
from lerobot.common.robots.UR5e_follower import URRobotLeRobot  # noqa: F401
from lerobot.common.cameras import (  # noqa: F401
    CameraConfig,  # noqa: F401
)
from lerobot.common.cameras.opencv.configuration_opencv import OpenCVCameraConfig  # noqa: F401
from lerobot.common.cameras.realsense.configuration_realsense import RealSenseCameraConfig  # noqa: F401
from lerobot.common.datasets.image_writer import safe_stop_image_writer
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from lerobot.common.datasets.utils import build_dataset_frame, hw_to_dataset_features
from lerobot.common.policies.factory import make_policy
from lerobot.common.policies.pretrained import PreTrainedPolicy
from lerobot.common.robots import (  # noqa: F401
    Robot,
    RobotConfig,
    koch_follower,
    make_robot_from_config,
    so100_follower,
    so101_follower,
)
#from lerobot.common.robots.UR5e_follower import URRobotLeRobot
from lerobot.common.teleoperators.VR import VR_leader  # noqa: F401
from lerobot.common.teleoperators import (  # noqa: F401
    Teleoperator,
    TeleoperatorConfig,
    koch_leader,
    make_teleoperator_from_config,
    so100_leader,
    so101_leader,
    VR_leader,
)
from lerobot.common.utils.control_utils import (
    init_keyboard_listener,
    is_headless,
    predict_action,
    sanity_check_dataset_name,
    sanity_check_dataset_robot_compatibility,
)
from lerobot.common.utils.robot_utils import busy_wait
from lerobot.common.utils.utils import (
    get_safe_torch_device,
    init_logging,
    log_say,
)
from lerobot.common.utils.visualization_utils import _init_rerun
from lerobot.configs import parser
from lerobot.configs.policies import PreTrainedConfig
from demo_modbus import hand_control,hand_start,hand_read
import socket
from scipy.spatial.transform import Rotation as R

def is_port_in_use(ip, port):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    try:
        result = s.connect_ex((ip, port))
        s.close()
        # 0 表示端口已被占用（有服务监听），111/10061等表示未监听
        return result == 0
    except Exception as e:
        print(f"Port check error: {e}")
        return False
class KeyboardListener:
    def __init__(self):
        self.robot1_enabled = False
        self.running = True
        self.robot1_reset = False
        self.hand_start = False
    def listen_for_keys(self):
        """监听键盘输入的线程函数"""
        print("键盘监听已启动！按 'a' 键启动机器人，按 'q' 键退出程序")
        while self.running:
            try:
                key = input().strip().lower()
                if key == 'a':
                    self.robot1_enabled = True
                    print("机器人已启动！")
                elif key == 'r':    
                    self.robot1_reset = True
                    print("机器人已重置！")
                elif key == 'z':
                    self.hand_start = True
                elif key == 'x':
                    self.hand_start = False
                    print("手部动作已停止！")
                elif key == 'q':
                    self.running = False
                    print("程序即将退出...")
                    break
            except KeyboardInterrupt:
                break
                
    def start_listening(self):
        """启动键盘监听线程"""
        self.listener_thread = threading.Thread(target=self.listen_for_keys, daemon=True)
        self.listener_thread.start()
        
    def is_robot1_enabled(self):
        """检查机器人是否被启用"""
        return self.robot1_enabled
    
    def is_robot1_reset(self):
        """检查机器人是否被重置"""
        # 这里可以添加具体的重置逻辑
        return self.robot1_reset
        
    def should_exit(self):
        """检查是否应该退出程序"""
        return not self.running

class VuerTeleop:
    """
    负责远程操作手部数据的采集与预处理，包括图像共享内存、预处理、重定向配置等。
    """
    def __init__(self, config_file_path):
        # 原始分辨率
        self.resolution = (720, 1280)
        # 裁剪宽高（此处为0，未裁剪）
        self.crop_size_w = 0
        self.crop_size_h = 0
        self.keyboard_listener = KeyboardListener()
        self.keyboard_listener.start_listening()
        # 裁剪后的分辨率
        self.resolution_cropped = (self.resolution[0]-self.crop_size_h, self.resolution[1]-2*self.crop_size_w)

        # 图像shape：(高, 2*宽, 3)，左右拼接
        self.img_shape = (self.resolution_cropped[0], 2 * self.resolution_cropped[1], 3)
        self.img_height, self.img_width = self.resolution_cropped[:2]

        # 创建共享内存用于图像传递
        self.shm = shared_memory.SharedMemory(create=True, size=np.prod(self.img_shape) * np.uint8().itemsize)
        self.img_array = np.ndarray((self.img_shape[0], self.img_shape[1], 3), dtype=np.uint8, buffer=self.shm.buf)
        image_queue = Queue()
        toggle_streaming = Event()

        # 初始化视频采集与预处理
        self.tv = OpenTeleVision(self.resolution_cropped, self.shm.name, image_queue, toggle_streaming, ngrok=False)
        self.processor = VuerPreprocessor()
        self.old_hand_pose = [1000, 1000, 1000, 1000, 1000, 1000]  # 初始化手部姿态
        # 设置重定向配置
        RetargetingConfig.set_default_urdf_dir('/home/hpx/vla/src/lerobot/lerobot/assets')
        with Path(config_file_path).open('r') as f:
            cfg = yaml.safe_load(f)
        left_retargeting_config = RetargetingConfig.from_dict(cfg['left'])
        right_retargeting_config = RetargetingConfig.from_dict(cfg['right'])
        self.left_retargeting = left_retargeting_config.build()
        self.right_retargeting = right_retargeting_config.build()
        self.start_flag1 = 0
        #self.robot2 = UR5e_arm(robot1_ip="192.168.31.2")
        self.robot2 = URRobotLeRobot()

    def step(self):
        """
        处理一帧数据，输出头部旋转矩阵、左右手位姿和关节角。
        """
        # 处理视频流，获得头部和手部的位姿矩阵
        head_mat, left_wrist_mat, right_wrist_mat, left_hand_mat, right_hand_mat = self.processor.process(self.tv)

        # 提取头部旋转矩阵
        head_rmat = head_mat[:3, :3]

        # 计算左手和右手的位姿（位置+四元数），并做平移修正
        left_pose = np.concatenate([left_wrist_mat[:3, 3] + np.array([-0.6, 0, 1.6]),
                                    rotations.quaternion_from_matrix(left_wrist_mat[:3, :3])[[1, 2, 3, 0]]])
        right_pose = np.concatenate([right_wrist_mat[:3, 3] + np.array([-0.6, 0, 1.6]),
                                     rotations.quaternion_from_matrix(right_wrist_mat[:3, :3])[[1, 2, 3, 0]]])
        # 通过重定向获得左右手的关节角，并按指定顺序排列
        left_qpos = self.left_retargeting.retarget(left_hand_mat[tip_indices])[[4, 5, 6, 7, 10, 11, 8, 9, 0, 1, 2, 3]]
        right_qpos = self.right_retargeting.retarget(right_hand_mat[tip_indices])[[4, 5, 6, 7, 10, 11, 8, 9, 0, 1, 2, 3]]

        return head_rmat, left_pose, right_pose, left_qpos, right_qpos,right_wrist_mat
    def correct_quaternion_and_get_rotvec(self,q_original):
        """
        修正坐标系反向问题并返回旋转向量
        :param q_original: 原始四元数 [w, x, y, z]
        :return: UR RTDE兼容的旋转向量 [Rx, Ry, Rz]（弧度制）
        """
        # 1. 构建绕Z轴180°的修正四元数
        #q_z180 = np.array([0, 0, 0, 0])  # [w=0, x=0, y=0, z=1]
        
        # 2. 四元数乘法：q_corrected = q_z180 * q_original
    #    r_orig = R.from_quat(q_original)
        #r_z180 = R.from_quat(q_z180)
        #r_corrected = r_z180 * r_orig  # 组合旋转
        #rotvec = r_corrected.as_rotvec()
    #    rotvec = r_orig.as_rotvec()
        rota1 = R.from_matrix(q_original[:3, :3])
        r_z90 = R.from_rotvec([-np.pi / 2, 0, 0])  # 绕 Z 轴旋转
        # 3. 组合旋转矩阵
        r_combined = r_z90 * rota1  # 注意旋转的顺序
        # 4. 将组合后的旋转矩阵转换回旋转向量
        combined_rotvec = r_combined.as_rotvec()
        combined_rotvec[0] = - combined_rotvec[0]
        combined_rotvec[2] = - combined_rotvec[2]
        return combined_rotvec
    def get_old_action(self):
        if self.start_flag1 == 0:
            head_rmat, left_pose, right_pose, left_qpos, right_qpos ,right_wrist_mat= self.step()
            old_quat = self.correct_quaternion_and_get_rotvec(right_wrist_mat)
            old_pose = [right_pose[0],right_pose[1],right_pose[2],old_quat[0],old_quat[1],old_quat[2]]
            joint_pos, old_tcp_pose = self.robot2.read()
            self.start_flag1 = 1
            return old_pose, old_tcp_pose

    def get_handpose(self,right_qpos):
        """控制手部动作"""
        # 这里可以添加手部控制的具体实现
        selected = [
            right_qpos[0],   # 1st
            right_qpos[2],   # 3rd
            right_qpos[4],   # 5th
            right_qpos[6],   # 7th
            right_qpos[8],   # 9th
            right_qpos[10]   # 11th
        ]
        
        # 映射前4个元素 (1.0-1.7 → 0-1000)
        mapped_first_part = []
        for value in selected[:4]:
            # 线性映射公式：y = (x - min) * (1000/(max-min))
            mapped_value = 1000 - (value - 1.0) * (1000 / 0.7)
            # 确保结果在[0,1000]范围内
            mapped_value = max(0, min(1000, mapped_value))
            mapped_first_part.append(mapped_value)
        
        # 映射后2个元素 (0-0.8 → 0-1000)
        mapped_second_part = []
        value = selected[4]  # 5th
        mapped_value = 1000 - value * (1000 / 1.3)
        mapped_value = max(0, min(1000, mapped_value))
        mapped_second_part.append(mapped_value)
        value = selected[5]  # 5th
        mapped_value = 1000 - value * (1000 / 0.8)
        mapped_value = max(0, min(1000, mapped_value))
        mapped_second_part.append(mapped_value)
        result = [
        int(mapped_first_part[2]),
        int(mapped_first_part[3]),
        int(mapped_first_part[1]),
        int(mapped_first_part[0]),
        int(mapped_second_part[1]),
        int(mapped_second_part[0])
        ]
        return result
    def get_action(self, events):
        if self.start_flag1 == 0:
            self.old_pose, self.old_tcp_pose = self.get_old_action()
            self.pre_pose = self.old_pose
        joint_pos, tcp_pose = self.robot2.read()
        head_rmat, left_pose, right_pose, left_qpos, right_qpos,right_wrist_mat = self.step()
        new_tcp_pose = list(tcp_pose)
        if (right_pose[0]-self.pre_pose[0]) > 0.003 or (right_pose[1]-self.pre_pose[1]) > 0.003 or (right_pose[2]-self.pre_pose[2]) > 0.003:
            new_tcp_pose[0] = tcp_pose[0] - (right_pose[0] - self.old_pose[0]) * 1 # x方向
            new_tcp_pose[1] = tcp_pose[1] - (right_pose[1] - self.old_pose[1]) * 0.5
            new_tcp_pose[2] = tcp_pose[2] + (right_pose[2] - self.old_pose[2]) * 0.5
        self.pre_pose = right_pose
        if new_tcp_pose[1] > (self.old_tcp_pose[1] + 0.15):
            new_tcp_pose[1] = self.old_tcp_pose[1] + 0.15
            print(f"new_tcp_pose[1] 超过0.5，已调整为 {new_tcp_pose[1]}")
        if new_tcp_pose[1] < (self.old_tcp_pose[1] - 0.16):
            new_tcp_pose[1] = self.old_tcp_pose[1] - 0.16     
            print(f"new_tcp_pose[1] 超过-0.5，已调整为 {new_tcp_pose[1]}")
        if new_tcp_pose[0] > (self.old_tcp_pose[0] + 0.4):
            new_tcp_pose[0] = self.old_tcp_pose[0] + 0.4
            print(f"new_tcp_pose[0] 超过0.4，已调整为 {new_tcp_pose[0]}")
        if new_tcp_pose[0] < (self.old_tcp_pose[0] - 0.4):
            new_tcp_pose[0] = self.old_tcp_pose[0] - 0.4  
            print(f"new_tcp_pose[0] 超过-0.4，已调整为 {new_tcp_pose[0]}")
        if (new_tcp_pose[0] < (self.old_tcp_pose[0] - 0.05)) and (new_tcp_pose[1] > (self.old_tcp_pose[1] + 0.12)):
            new_tcp_pose[1] = (self.old_tcp_pose[1] + 0.12)
            print(f"new_tcp_pose[0] 和 new_tcp_pose[1] 超过限制，已调整为, {new_tcp_pose[1]}")
        if new_tcp_pose[2] < 0.10:
            new_tcp_pose[2] = 0.10
        if new_tcp_pose[2] > 0.25:
            new_tcp_pose[2] = 0.25
        quat = right_pose[3:7]  # x, y, z, w
        new_quat = [quat[3], -quat[0], -quat[1], quat[2]]
        current_q = joint_pos  # 返回当前6个关节角的列表
        final_joint = self.robot2.robot1.getInverseKinematics(
            new_tcp_pose,          # 第一个参数必须是目标位姿列表，不可用target_pose=
            current_q,              # 第二个参数为q_near列表，不可为None
            max_position_error=10, 
            max_orientation_error=10
        )
        if final_joint[3] > -0.5:
            final_joint[3] = -0.6
        elif final_joint[3] < -3.6:
            final_joint[3] = -3.5

        hand_pose = self.get_handpose(right_qpos)
        #print(f'control signal:66666666666666: {new_control}')                
        action = {
            "joint_1.pos": final_joint[0],
            "joint_2.pos": final_joint[1],
            "joint_3.pos": final_joint[2],
            "joint_4.pos": final_joint[3],
            "joint_5.pos": final_joint[4],
            "joint_6.pos": final_joint[5],
            "tcp_1.pos": new_tcp_pose[0],
            "tcp_2.pos": new_tcp_pose[1],
            "tcp_3.pos": new_tcp_pose[2],
            "tcp_4.pos": new_tcp_pose[3],
            "tcp_5.pos": new_tcp_pose[4],
            "tcp_6.pos": new_tcp_pose[5],
            "hand_1.pos": hand_pose[0],
            "hand_2.pos": hand_pose[1],
            "hand_3.pos": hand_pose[2],
            "hand_4.pos": hand_pose[3],
            "hand_5.pos": hand_pose[4],
            "hand_6.pos": hand_pose[5],
        }


        return action
        
@dataclass
class DatasetRecordConfig:
    # Dataset identifier. By convention it should match '{hf_username}/{dataset_name}' (e.g. `lerobot/test`).
    repo_id: str
    # A short but accurate description of the task performed during the recording (e.g. "Pick the Lego block and drop it in the box on the right.")
    single_task: str
    # Root directory where the dataset will be stored (e.g. 'dataset/path').
    root: str | Path | None = None
    # Limit the frames per second.
    fps: int = 30
    # Number of seconds for data recording for each episode.
    episode_time_s: int | float = 60
    # Number of seconds for resetting the environment after each episode.
    reset_time_s: int | float = 60
    # Number of episodes to record.
    num_episodes: int = 50
    # Encode frames in the dataset into video
    video: bool = True
    # Upload dataset to Hugging Face hub.
    push_to_hub: bool = True
    # Upload on private repository on the Hugging Face hub.
    private: bool = False
    # Add tags to your dataset on the hub.
    tags: list[str] | None = None
    # Number of subprocesses handling the saving of frames as PNG. Set to 0 to use threads only;
    # set to ≥1 to use subprocesses, each using threads to write images. The best number of processes
    # and threads depends on your system. We recommend 4 threads per camera with 0 processes.
    # If fps is unstable, adjust the thread count. If still unstable, try using 1 or more subprocesses.
    num_image_writer_processes: int = 0
    # Number of threads writing the frames as png images on disk, per camera.
    # Too many threads might cause unstable teleoperation fps due to main thread being blocked.
    # Not enough threads might cause low camera fps.
    num_image_writer_threads_per_camera: int = 4

    def __post_init__(self):
        if self.single_task is None:
            raise ValueError("You need to provide a task as argument in `single_task`.")


@dataclass
class RecordConfig:
    srobot: RobotConfig
    #robot = make_robot_from_config(cfg.robot)
    #robot: Optional[RobotConfig] = None
    dataset: DatasetRecordConfig
    # Whether to control the robot with a teleoperator
    teleop: TeleoperatorConfig | None = None
    # Whether to control the robot with a policy
    policy: PreTrainedConfig | None = None
    # Display all cameras on screen
    display_data: bool = False
    # Use vocal synthesis to read events.
    play_sounds: bool = True
    # Resume recording on an existing dataset.
    resume: bool = False

    def __post_init__(self):
        # HACK: We parse again the cli args here to get the pretrained path if there was one.
        policy_path = parser.get_path_arg("policy")
        if policy_path:
            cli_overrides = parser.get_cli_overrides("policy")
            self.policy = PreTrainedConfig.from_pretrained(policy_path, cli_overrides=cli_overrides)
            self.policy.pretrained_path = policy_path

        if self.teleop is None and self.policy is None:
            raise ValueError("Choose a policy, a teleoperator or both to control the robot")

    @classmethod
    def __get_path_fields__(cls) -> list[str]:
        """This enables the parser to load config from the policy using `--policy.path=local/dir`"""
        return ["policy"]


@safe_stop_image_writer
def record_loop(
    robot: Robot,
    events: dict,
    fps: int,
    dataset: LeRobotDataset | None = None,
    teleop: Teleoperator | None = None,
    policy: PreTrainedPolicy | None = None,
    control_time_s: int | None = None,
    single_task: str | None = None,
    display_data: bool = False,
):
    if dataset is not None and dataset.fps != fps:
        raise ValueError(f"The dataset fps should be equal to requested fps ({dataset.fps} != {fps}).")

    # if policy is given it needs cleaning up
    # if policy is not None:
    #     policy.reset()
    old_control = [0,0,0,0,0,0,0,0]
    timestamp = 0
    start_episode_t = time.perf_counter()
    while timestamp < control_time_s:
        start_loop_t = time.perf_counter()

        if events["exit_early"]:
            events["exit_early"] = False
            break

        observation = robot.get_observation()
        #hand_pose = [observation[f"hand_{i+1}.pos"] for i in range(6)]
        if policy is not None or dataset is not None:
            observation_frame = build_dataset_frame(dataset.features, observation, prefix="observation")

        if policy is not None:
            action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
            action = {key: action_values[i].item() for i, key in enumerate(robot.action_features)}
        elif policy is None and teleop is not None:
            action = teleop.get_action(events)
            if action is None:
                logging.info("Teleoperator returned None action, skipping this loop iteration.")
                continue
        else:
            logging.info(
                "No policy or teleoperator provided, skipping action generation."
                "This is likely to happen when resetting the environment without a teleop device."
                "The robot won't be at its rest position at the start of the next episode."
            )
            continue

        # Action can eventually be clipped using `max_relative_target`,
        # so action actually sent is saved in the dataset.
        sent_action = robot.send_action(action)

        if dataset is not None:
            action_frame = build_dataset_frame(dataset.features, sent_action, prefix="action")
            frame = {**observation_frame, **action_frame}
            dataset.add_frame(frame, task=single_task)

        if display_data:
            for obs, val in observation.items():
                if isinstance(val, float):
                    rr.log(f"observation.{obs}", rr.Scalar(val))
                elif isinstance(val, np.ndarray):
                    rr.log(f"observation.{obs}", rr.Image(val), static=True)
            for act, val in action.items():
                if isinstance(val, float):
                    rr.log(f"action.{act}", rr.Scalar(val))

        dt_s = time.perf_counter() - start_loop_t
        busy_wait(1 / fps - dt_s)

        timestamp = time.perf_counter() - start_episode_t


@parser.wrap()
def record(cfg: RecordConfig,teleop,listener, events) -> LeRobotDataset:
    init_logging()
    logging.info(pformat(asdict(cfg)))
    if cfg.display_data:
        _init_rerun(session_name="recording")

    #robot = make_robot_from_config(cfg.robot)
    #robot = URRobotLeRobot()
    
    #teleop = make_teleoperator_from_config(cfg.teleop) if cfg.teleop is not None else None
    teleop = teleop
    robot = teleop.robot2
    action_features = hw_to_dataset_features(robot.action_features, "action", cfg.dataset.video)
    obs_features = hw_to_dataset_features(robot.observation_features, "observation", cfg.dataset.video)
    dataset_features = {**action_features, **obs_features}

    if cfg.resume:
        dataset = LeRobotDataset(
            cfg.dataset.repo_id,
            root=cfg.dataset.root,
        )

        if hasattr(robot, "cameras") and len(robot.cameras) > 0:
            dataset.start_image_writer(
                num_processes=cfg.dataset.num_image_writer_processes,
                num_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
            )
        sanity_check_dataset_robot_compatibility(dataset, robot, cfg.dataset.fps, dataset_features)
    else:
        # Create empty dataset or load existing saved episodes
        sanity_check_dataset_name(cfg.dataset.repo_id, cfg.policy)
        dataset = LeRobotDataset.create(
            cfg.dataset.repo_id,
            cfg.dataset.fps,
            root=cfg.dataset.root,
            robot_type=robot.name,
            features=dataset_features,
            use_videos=cfg.dataset.video,
            image_writer_processes=cfg.dataset.num_image_writer_processes,
            image_writer_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
        )

    # Load pretrained policy
    policy = None if cfg.policy is None else make_policy(cfg.policy, ds_meta=dataset.meta)

    robot.connect()
    # if teleop is not None:
    #     teleop.connect()

    

    recorded_episodes = 0
    while recorded_episodes < cfg.dataset.num_episodes and not events["stop_recording"]:
        log_say(f"Recording episode {dataset.num_episodes}", cfg.play_sounds)
        record_loop(
            robot=robot,
            events=events,
            fps=cfg.dataset.fps,
            teleop=teleop,
            policy=policy,
            dataset=dataset,
            control_time_s=cfg.dataset.episode_time_s,
            single_task=cfg.dataset.single_task,
            display_data=cfg.display_data,
        )

        # Execute a few seconds without recording to give time to manually reset the environment
        # Skip reset for the last episode to be recorded
        if not events["stop_recording"] and (
            (recorded_episodes < cfg.dataset.num_episodes - 1) or events["rerecord_episode"]
        ):
            log_say("Reset the environment", cfg.play_sounds)
            print("Reset the environment9999999999999999999999999999999999999999999")
            events["control"] = [0, 0, 0, 0, 0, 0, 0, 0]
            joint_pos = [0, -2.1, -2, -2.1, 0, 0]
            robot.robot1.moveJ(joint_pos, 0.3, 0.5, False)
            record_loop(
                robot=robot,
                events=events,
                fps=cfg.dataset.fps,
                teleop=teleop,
                control_time_s=cfg.dataset.reset_time_s,
                single_task=cfg.dataset.single_task,
                display_data=cfg.display_data,
            )

        if events["rerecord_episode"]:
            log_say("Re-record episode", cfg.play_sounds)
            events["rerecord_episode"] = False
            events["exit_early"] = False
            dataset.clear_episode_buffer()
            continue

        dataset.save_episode()
        recorded_episodes += 1

    log_say("Stop recording", cfg.play_sounds, blocking=True)

    robot.disconnect()
    # if teleop is not None:
    #     teleop.disconnect()

    if not is_headless() and listener is not None:
        listener.stop()

    # if cfg.dataset.push_to_hub:
    #     dataset.push_to_hub(tags=cfg.dataset.tags, private=cfg.dataset.private)

    log_say("Exiting", cfg.play_sounds)
    return dataset


if __name__ == "__main__":
    #robot1= URRobotLeRobot()
    teleoperator = VuerTeleop('/home/hpx/vla/src/lerobot/lerobot/inspire_hand.yml')
    #ser = hand_start()
    robot1 = teleoperator.robot2
    # keyboard_listener = KeyboardListener()
    # keyboard_listener.start_listening()

    listener, events = init_keyboard_listener()
    joint_pos = [-0.6, -1.8, -2.1, -2.37, -0.6, -0.1]
    #joint_pos, tcp_pose = robot1.read()
    #joint_pos[4] += 0.3
    robot1.robot1.moveJ(joint_pos, 0.3, 0.5, False)
    # robot_ip = "192.168.31.2"
    # rtde_port = 30004
    # if is_port_in_use(robot_ip, rtde_port):
    #     print(f"警告：{robot_ip}:{rtde_port} 端口已被占用，RTDE 可能已被其他服务占用！")
    # else:
    #     print(f"{robot_ip}:{rtde_port} 端口未被占用，可以尝试连接。")
    #robot1= UR5e_arm()
    start_flag = 0
    try:
        while True:
            if events["stop_recording"]:
                break


            if events["restart_arm"]:
                print("机器人已重置！")
                # 重置机器人到初始位置
                joint_pos, tcp_pose = robot1.read()
                print(f"当前关节角度: {joint_pos}")
                print(f"当前TCP位姿: {tcp_pose}")
                arm_pose = tcp_pose.copy()
                arm_pose[2] -= 0.2
                #joint_pos[3] +=  -0.4
                joint_pos = [0, -2.1, -2, -2.1, 0, 0]
                robot1.robot1.moveJ(joint_pos, 0.3, 0.5, False)
                #robot.move_safety(arm_pose)
                #time.sleep(1)
                break
            
            if events["start_record"] and start_flag == 0:
                start_flag = 1
            
            else:
                print("机器人未启动，等待按 'z' 键...")
                
                #robot1.robot1.moveL(target_tcp, speed=0.1, acceleration=0.5 asynchronous=False)
            # 获取当前帧的头部和手部数据
            if start_flag == 0:
                # joint_pos, tcp_pose = robot1.read()
                # print(f"当前关节角度: {joint_pos}")
                # print(f"当前TCP位姿: {tcp_pose}")
                head_rmat, left_pose, right_pose, left_qpos, right_qpos ,right_wrist_mat= teleoperator.step()
                print(f'Right Hand Pose: {right_pose[3:7]}')

                # print(f'计数数据：{events["w_count"]}')
            elif start_flag == 1:
                time.sleep(2)  # 等待一段时间以确保数据稳定
                start_flag = 2
                #head_rmat, left_pose, right_pose, left_qpos, right_qpos = teleoperator.step()
               # global old_pose = right_pose.copy()
            elif start_flag == 2:
                record(teleop = teleoperator,listener = listener, events=events)

                #head_rmat, left_pose, right_pose, left_qpos, right_qpos,right_wrist_mat = teleoperator.step()
                #rota1 = R.from_matrix(right_wrist_mat[:3, :3]).as_rotvec()
                # rota1 = R.from_matrix(right_wrist_mat[:3, :3])
                # r_z90 = R.from_rotvec([-np.pi / 2, 0, 0])  # 绕 Z 轴旋转
                # # 3. 组合旋转矩阵
                # r_combined = r_z90 * rota1  # 注意旋转的顺序
                # # 4. 将组合后的旋转矩阵转换回旋转向量
                # combined_rotvec = r_combined.as_rotvec()
                # #print(f"捕捉的手部旋转向量：{combined_rotvec}")
                # hands = teleoperator.get_handpose(right_qpos)
                # action = teleoperator.get_action(events)
                # #time.sleep(0.3)
                # #action = robot1.get_action(events)
                # rota2 = [action["tcp_1.pos"],action["tcp_2.pos"],action["tcp_3.pos"]]
                # rota3 = right_pose[:3]
                # #print(f"机械臂现在的旋转：{rota2}")
                # tcp_targets = [action[f"tcp_{i+1}.pos"] for i in range(6)]
                # velocity = 0.3
                # acceleration = 1
                # robot1.robot1.moveL(tcp_targets, velocity, acceleration, False)
                #time.sleep(0.5)
                
                #hand_control(ser,hands)
                #break
            #time.sleep(0.5)
    # keyboard_listener = KeyboardListener()
    # keyboard_listener.start_listening()
    # robot1_ip = "192.168.31.2"
    # while True:
    #     try:
    #         print(f"尝试连接 RTDE 控制端口（{robot1_ip}:30004）...")
    #         robot1 = rtde_control.RTDEControlInterface(robot1_ip)
    #         robot1.endFreedriveMode()
    #         print("连接成功！")
    #         break
    #     except Exception as e:
    #         print(e)
    #         print(robot1_ip)
    #         print("Failed to connect to the robot. Will retry in 1 seconds...")
    #         time.sleep(1)
    # r_inter = rtde_receive.RTDEReceiveInterface(robot1_ip)

    except KeyboardInterrupt:
        # 退出时释放资源
        exit(0)
